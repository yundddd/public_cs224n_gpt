{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f751939",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from models.gpt2_moe import GPT2MoEModel\n",
    "from optimizer import AdamW\n",
    "from models.gpt2 import GPT2Model\n",
    "from evaluation import model_eval_paraphrase, model_test_paraphrase, model_eval_sonnet\n",
    "from datasets import (\n",
    "    ParaphraseDetectionDataset,\n",
    "    ParaphraseDetectionTestDataset,\n",
    "    SonnetsDataset,\n",
    "    load_paraphrase_data\n",
    ")\n",
    "from transformers import GPT2Tokenizer\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8093fb1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "import wandb\n",
    "import os\n",
    "# nobody cares about security\n",
    "os.environ['WANDB_API_KEY'] = 'd8e70c9cb01a88ace48a2ec6d18bd9e9be24c73b'\n",
    "os.environ['WANDB_ENTITY'] = 'yundddd-stanford-university'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d548899",
   "metadata": {},
   "outputs": [],
   "source": [
    "TQDM_DISABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3845a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Fix the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672eca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=11711):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEGPT(nn.Module):\n",
    "    \"\"\"Your GPT-2 Model designed for paraphrase detection.\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2MoEModel.from_pretrained(\n",
    "            model=args.model_size, d=args.d, l=args.l, num_heads=args.num_heads,\n",
    "            num_moe_layers=args.num_moe_layers,\n",
    "            expert_hidden_size=args.expert_hidden_size,\n",
    "            num_experts=args.num_experts,\n",
    "            aux_loss_weight=args.aux_loss_weight)\n",
    "        # Paraphrase detection has two outputs: 1 (yes) or 0 (no).\n",
    "        self.paraphrase_detection_head = nn.Linear(args.d, 2)\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Only fine tune the MoE layers.\n",
    "        for param in self.gpt.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gpt.gptmoe_layers.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        gpt_output = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_token_hidden_state = gpt_output[\"last_token\"]\n",
    "        classification_logits = self.paraphrase_detection_head(last_token_hidden_state)\n",
    "\n",
    "        last_hidden_state = gpt_output[\"last_hidden_state\"]\n",
    "        next_token_logits = self.gpt.hidden_state_to_token(last_hidden_state)\n",
    "        return {\"classification_logits\": classification_logits,\n",
    "                \"next_token_logits\": next_token_logits,\n",
    "                \"aux_loss\": gpt_output[\"aux_loss\"]}\n",
    "\n",
    "    def get_device(self):\n",
    "        for param in self.gpt.parameters():\n",
    "            return param.device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, encoding, temperature=0.7, top_p=0.9, max_length=128):\n",
    "        \"\"\"\n",
    "        Generates an original sonnet using top-p sampling and softmax temperature.\n",
    "\n",
    "        TODO: this is probably not ideal. You can look at hugging face's model.generate(...) function for inspiration.\n",
    "        In particular, generating multiple sequences and choosing the best with beam search is one avenue. Top_k is another;\n",
    "        there are many.\n",
    "        \"\"\"\n",
    "        token_ids = encoding.to(self.get_device())\n",
    "        attention_mask = torch.ones(\n",
    "            token_ids.shape, dtype=torch.int64).to(\n",
    "            self.get_device())\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass to get logits\n",
    "            output = self.forward(token_ids, attention_mask)\n",
    "            logits_sequence = output[\"next_token_logits\"]\n",
    "            # Apply temperature scaling\n",
    "            logits_last_token = logits_sequence[:, -1, :] / temperature\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.nn.functional.softmax(logits_last_token, dim=-1)\n",
    "\n",
    "            # Top-p (nucleus) sampling\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            top_p_mask = cumulative_probs <= top_p\n",
    "            # Shift mask right for proper thresholding\n",
    "            top_p_mask[..., 1:] = top_p_mask[..., :-1].clone()\n",
    "            top_p_mask[..., 0] = True  # Always include the highest probability token\n",
    "            filtered_probs = sorted_probs * top_p_mask  # Zero out unlikely tokens\n",
    "            # Normalize probabilities\n",
    "            filtered_probs /= filtered_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # Sample from filtered distribution\n",
    "            sampled_index = torch.multinomial(filtered_probs, 1)\n",
    "            sampled_token = sorted_indices.gather(dim=-1, index=sampled_index)\n",
    "\n",
    "            # Stop if end-of-sequence token is reached\n",
    "            if sampled_token.item() == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Append sampled token\n",
    "            token_ids = torch.cat([token_ids, sampled_token], dim=1)\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones(\n",
    "                (1, 1), dtype=torch.int64).to(self.get_device())], dim=1)\n",
    "\n",
    "        generated_output = self.tokenizer.decode(\n",
    "            token_ids[0].cpu().numpy().tolist())[3:]\n",
    "        return token_ids, generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5929cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, args, filepath):\n",
    "    save_info = {\n",
    "        'model': model.state_dict(),\n",
    "        'optim': optimizer.state_dict(),\n",
    "        'args': args,\n",
    "        'system_rng': random.getstate(),\n",
    "        'numpy_rng': np.random.get_state(),\n",
    "        'torch_rng': torch.random.get_rng_state(),\n",
    "    }\n",
    "\n",
    "    torch.save(save_info, filepath)\n",
    "    print(f\"save the model to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03bd754",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(labels):\n",
    "    label_map = {3919: 1, 8505: 0}\n",
    "    return torch.tensor([label_map[label.item()] for label in labels], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24105df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paraphrase_task_dataloaders(args):\n",
    "    para_train_data = load_paraphrase_data(args.para_train)[:70000]\n",
    "    para_dev_data = load_paraphrase_data(args.para_dev)[:10000]\n",
    "\n",
    "    para_train_data = ParaphraseDetectionDataset(para_train_data, args)\n",
    "    para_dev_data = ParaphraseDetectionDataset(para_dev_data, args)\n",
    "\n",
    "    para_train_dataloader = DataLoader(\n",
    "        para_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "        collate_fn=para_train_data.collate_fn)\n",
    "    para_dev_dataloader = DataLoader(\n",
    "        para_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "        collate_fn=para_dev_data.collate_fn)\n",
    "\n",
    "    return {\"train\": para_train_dataloader, \"dev\": para_dev_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sonnet_task_dataloaders(args):\n",
    "    train_sonnet_dataset = SonnetsDataset(args.sonnet_train_path)\n",
    "    train_sonnet_dataloader = DataLoader(\n",
    "        train_sonnet_dataset, shuffle=True, batch_size=args.batch_size,\n",
    "        collate_fn=train_sonnet_dataset.collate_fn)\n",
    "\n",
    "    dev_held_out_sonnet_dataset = SonnetsDataset(args.sonnet_dev_held_out_path)\n",
    "\n",
    "    return {\"train\": train_sonnet_dataloader,\n",
    "            \"dev_held_out\": dev_held_out_sonnet_dataset,\n",
    "            \"dev_label_path\": args.sonnet_dev_label_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullUsageLoader:\n",
    "    def __init__(self, main_loader, aux_loader):\n",
    "        self.main_loader = iter(main_loader)  # Iterate through main data normally\n",
    "        self.aux_loader = cycle(aux_loader)  # Cycle through auxiliary data\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.main_loader)\n",
    "\n",
    "    def __next__(self):\n",
    "        main_batch = next(self.main_loader)\n",
    "        aux_batch = next(self.aux_loader)\n",
    "\n",
    "        return {'main': main_batch, 'aux': aux_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    \"\"\"Train GPT-2 for paraphrase detection on the Quora dataset.\"\"\"\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    # paraphrase task\n",
    "    para_task_dataloaders = get_paraphrase_task_dataloaders(args)\n",
    "    sonnet_task_dataloaders = get_sonnet_task_dataloaders(args)\n",
    "\n",
    "    args = add_arguments(args)\n",
    "    model = MoEGPT(args)\n",
    "    model = model.to(device)\n",
    "\n",
    "    lr = args.lr\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.)\n",
    "\n",
    "    best_para_dev_acc = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        task1_train_loss = 0\n",
    "        task2_train_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        combined_loader = FullUsageLoader(\n",
    "            para_task_dataloaders[\"train\"],\n",
    "            sonnet_task_dataloaders[\"train\"])\n",
    "        with tqdm(iterable=combined_loader, total=len(combined_loader), desc=f\"Epoch {epoch+1}/{args.epochs}\") as pbar:\n",
    "            for batch in pbar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                main_batch = batch['main']\n",
    "                aux_batch = batch['aux']\n",
    "\n",
    "                # task 1\n",
    "                b_ids, b_mask, labels = main_batch['token_ids'], main_batch['attention_mask'], main_batch['labels'].flatten(\n",
    "                )\n",
    "                b_ids = b_ids.to(device)\n",
    "                b_mask = b_mask.to(device)\n",
    "                # Map labels to 0 and 1\n",
    "                mapped_labels = map_labels(labels).to(device)\n",
    "                output = model(b_ids, b_mask)\n",
    "                task1_loss = F.cross_entropy(\n",
    "                    output[\"classification_logits\"],\n",
    "                    mapped_labels, reduction='mean')\n",
    "                task1_aux_loss = output[\"aux_loss\"]\n",
    "\n",
    "                # task 2\n",
    "                b_ids, b_mask = aux_batch['token_ids'], aux_batch['attention_mask']\n",
    "                b_ids = b_ids.to(device)\n",
    "                b_mask = b_mask.to(device)\n",
    "                output = model(b_ids, b_mask)\n",
    "                logits = output[\"next_token_logits\"]\n",
    "                logits = rearrange(logits[:, :-1].contiguous(), 'b t d -> (b t) d')\n",
    "                # Ignore the first token to compose the labels.\n",
    "                labels = b_ids[:, 1:].contiguous().flatten()\n",
    "                task2_loss = F.cross_entropy(logits, labels, reduction='mean')\n",
    "                task2_aux_loss = output[\"aux_loss\"]\n",
    "\n",
    "                total_loss = 0.3*(task1_loss + task1_aux_loss) + \\\n",
    "                    0.7 * (task2_loss + task2_aux_loss)\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                task1_train_loss += task1_loss.item() + task1_aux_loss.item()\n",
    "                task2_train_loss += task2_loss.item() + task2_aux_loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                pbar.set_postfix({\n",
    "                    'Loss_Main': f'{task1_loss.item():.4f}',\n",
    "                    'Loss_Aux': f'{task2_loss.item():.4f}'\n",
    "                })\n",
    "\n",
    "        task1_train_loss = task1_train_loss/num_batches\n",
    "        task2_train_loss = task2_train_loss/num_batches\n",
    "\n",
    "        para_dev_acc, dev_f1, *_ = model_eval_paraphrase(\n",
    "            para_task_dataloaders[\"dev\"], model, device)\n",
    "\n",
    "        sonnet_dev_acc = model_eval_sonnet(\n",
    "            sonnet_task_dataloaders[\"dev_held_out\"],\n",
    "            sonnet_task_dataloaders[\"dev_label_path\"],\n",
    "            model, device, args.temperature, args.top_p)\n",
    "        \n",
    "        if para_dev_acc > best_para_dev_acc:\n",
    "            save_model(model, optimizer, args, args.filepath)\n",
    "            best_para_dev_acc = para_dev_acc\n",
    "\n",
    "        print(\n",
    "            f\"paraphrase_loss: {task1_train_loss:.3f}, sonnet_loss: {task2_train_loss:.3f} para dev acc :: {para_dev_acc:.3f}, sonnet dev acc :: {sonnet_dev_acc:.3f}\")\n",
    "        wandb.log({\n",
    "            \"paraphrase_loss\": task1_train_loss,\n",
    "            \"sonnet_loss\": task2_train_loss,\n",
    "            \"para_dev_acc\": para_dev_acc, \"sonnet_dev_acc\": sonnet_dev_acc,\n",
    "            \"epoch\": epoch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f005101",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(args):\n",
    "    \"\"\"Evaluate your model on the dev and test datasets; save the predictions to disk.\"\"\"\n",
    "    device = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n",
    "    saved = torch.load(args.filepath)\n",
    "\n",
    "    model = MoEGPT(saved['args'])\n",
    "    model.load_state_dict(saved['model'])\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Loaded model to test from {args.filepath}\")\n",
    "\n",
    "    para_dev_data = load_paraphrase_data(args.para_dev)\n",
    "    para_test_data = load_paraphrase_data(args.para_test, split='test')\n",
    "\n",
    "    para_dev_data = ParaphraseDetectionDataset(para_dev_data, args)\n",
    "    para_test_data = ParaphraseDetectionTestDataset(para_test_data, args)\n",
    "\n",
    "    para_dev_dataloader = DataLoader(\n",
    "        para_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "        collate_fn=para_dev_data.collate_fn)\n",
    "    para_test_dataloader = DataLoader(\n",
    "        para_test_data, shuffle=True, batch_size=args.batch_size,\n",
    "        collate_fn=para_test_data.collate_fn)\n",
    "\n",
    "    dev_para_acc, _, dev_para_y_pred, _, dev_para_sent_ids = model_eval_paraphrase(\n",
    "        para_dev_dataloader,\n",
    "        model,\n",
    "        device)\n",
    "    print(f\"dev paraphrase acc :: {dev_para_acc:.3f}\")\n",
    "    test_para_y_pred, test_para_sent_ids = model_test_paraphrase(\n",
    "        para_test_dataloader, model, device)\n",
    "\n",
    "    with open(args.para_dev_out, \"w+\") as f:\n",
    "        f.write(\"id \\t Predicted_Is_Paraphrase \\n\")\n",
    "        for p, s in zip(dev_para_sent_ids, dev_para_y_pred):\n",
    "            f.write(f\"{p}, {s} \\n\")\n",
    "\n",
    "    with open(args.para_test_out, \"w+\") as f:\n",
    "        f.write(\"id \\t Predicted_Is_Paraphrase \\n\")\n",
    "        for p, s in zip(test_para_sent_ids, test_para_y_pred):\n",
    "            f.write(f\"{p}, {s} \\n\")\n",
    "\n",
    "    wandb.log({\"best dev\": dev_para_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e362ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--para_train\", type=str, default=\"data/quora-train.csv\")\n",
    "    parser.add_argument(\"--para_dev\", type=str, default=\"data/quora-dev.csv\")\n",
    "    parser.add_argument(\"--para_test\", type=str, default=\"data/quora-test-student.csv\")\n",
    "    parser.add_argument(\"--para_dev_out\", type=str,\n",
    "                        default=\"predictions/para-dev-output.csv\")\n",
    "    parser.add_argument(\"--para_test_out\", type=str,\n",
    "                        default=\"predictions/para-test-output.csv\")\n",
    "\n",
    "    parser.add_argument(\"--sonnet_train_path\", type=str,\n",
    "                        default=\"data/sonnets_train.txt\")\n",
    "    parser.add_argument(\"--sonnet_dev_held_out_path\", type=str,\n",
    "                        default=\"data/sonnets_dev_held_out.txt\")\n",
    "    parser.add_argument(\"--sonnet_dev_label_path\", type=str,\n",
    "                        default=\"data/sonnets_dev_label.txt\")\n",
    "    parser.add_argument(\"--held_out_sonnet_path\", type=str,\n",
    "                        default=\"data/sonnets_held_out.txt\")\n",
    "    parser.add_argument(\"--sonnet_out\", type=str,\n",
    "                        default=\"predictions/generated_sonnets.txt\")\n",
    "    parser.add_argument(\"--temperature\", type=float,\n",
    "                        help=\"softmax temperature.\", default=1.2)\n",
    "    parser.add_argument(\n",
    "        \"--top_p\", type=float,\n",
    "        help=\"Cumulative probability distribution for nucleus sampling.\", default=0.9)\n",
    "\n",
    "    parser.add_argument(\"--seed\", type=int, default=11711)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--use_gpu\", action='store_true')\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", help='sst: 64, cfimdb: 8 can fit a 12GB GPU', type=int,\n",
    "        default=8)\n",
    "    parser.add_argument(\"--lr\", type=float, help=\"learning rate\", default=1e-5)\n",
    "    parser.add_argument(\n",
    "        \"--model_size\", type=str,\n",
    "        help=\"The model size as specified on hugging face. DO NOT use the xl model.\",\n",
    "        choices=['gpt2', 'gpt2-medium', 'gpt2-large'],\n",
    "        default='gpt2')\n",
    "\n",
    "    parser.add_argument(\"--num_moe_layers\", type=int, default=1)\n",
    "    parser.add_argument(\"--expert_hidden_size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--num_experts\", type=int, default=4)\n",
    "    parser.add_argument(\"--aux_loss_weight\", type=float, default=0.01)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arguments(args):\n",
    "    \"\"\"Add arguments that are deterministic on model size.\"\"\"\n",
    "    if args.model_size == 'gpt2':\n",
    "        args.d = 768\n",
    "        args.l = 12\n",
    "        args.num_heads = 12\n",
    "    elif args.model_size == 'gpt2-medium':\n",
    "        args.d = 1024\n",
    "        args.l = 24\n",
    "        args.num_heads = 16\n",
    "    elif args.model_size == 'gpt2-large':\n",
    "        args.d = 1280\n",
    "        args.l = 36\n",
    "        args.num_heads = 20\n",
    "    else:\n",
    "        raise Exception(f'{args.model_size} is not supported.')\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    args.filepath = f'{args.epochs}-{args.lr}-moe.pt'  # Save path.\n",
    "    seed_everything(args.seed)  # Fix the seed for reproducibility.\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"cs224n\",\n",
    "        config=args,\n",
    "        name=\"MoE-\" + datetime.now().strftime(\"%m-%d %H:%M:%S \")\n",
    "    )\n",
    "    wandb.run.log_code(include_fn=lambda path: path.endswith(\".py\"))\n",
    "    train(args)\n",
    "    test(args)\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
